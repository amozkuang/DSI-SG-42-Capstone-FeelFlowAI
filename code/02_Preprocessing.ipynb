{"cells":[{"cell_type":"markdown","metadata":{"id":"Ae9WCSYw79Ed"},"source":["<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n","\n","\n","# DSI-SG-42 Capstone Project:\n","### FeelFlow AI: Decoding Emotions, Advancing Patient Support\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["### **Background**\n","\n","In Singapore, the urgency to address mental health issues among younger generations, particularly GenZ and millennials, is critical due to increasing pressures from work, school, and personal relationships leading to anxiety, depression, and substance abuse. Recognizing this, the Ministry of Health and AI Singapore (NUS) have initiated the \"Mental Health with AI\" Seminar to integrate AI technologies with clinical practices, enhancing therapeutic processes.\n","\n","The aims of this study is to develop a real-time emotion predictor app. The objective is to alleviate the layer of assessing patients' emotional well-being, which is crucial in enabling a more accurate diagnosis and treatment from. The app is in its beta stages, but seeks to be presented at the seminar for. Further discussions to adoption and integration into pre-existing app/softwares can be opened during this seminar. \n","\n","### **Problem Statement**\n","##### *Where discerning peopleâ€™s emotion can sometimes be an unnerving guessing game. How can clinicians use speech emotion recognition technology to accurately assess patients' emotional well-being, thereby improving diagnosis and treatment outcomes?*"]},{"cell_type":"markdown","metadata":{},"source":["### **Table of Contents**\n","\n","### 2. [Preprocessing the Data](#preprocessing-the-data)\n","   #### 2.1 [Data Augmentation](#data-augmentation)\n","   ##### 2.1.1 [CREMA-D (Seen)](#crema-d-seen)\n","   ##### 2.1.2 [YouTube dataset (Unseen)](#youtube-dataset-unseen)\n","   ##### 2.1.3 [ESD data (Seen + Unseen)](#esd-data-seen-unseen)\n","   ##### 2.1.3.1 [ESD (Seen)](#esd-seen)\n","   ##### 2.1.3.2 [ESD (Unseen)](#esd-unseen)\n","   ##### 2.1.4 [TESS dataset (Seen)](#tess-dataset-seen)\n","   #### 2.2 [Label Mapping](#label-mapping)\n","   ##### 2.2.1 [CREMA-D (Seen)](#crema-d-seen-label)\n","   ##### 2.2.2 [ESD](#esd-label)\n","   ##### 2.2.2.1 [ESD (Seen)](#esd-seen-label)\n","   ##### 2.2.2.2 [ESD (Unseen)](#esd-unseen-label)\n","   ##### 2.2.3 [TESS dataset (Seen)](#tess-dataset-seen-label)\n","   #### 2.3 [Feature Extraction](#feature-extraction)\n","   ##### 2.3.1 [CREMA-D (Seen)](#crema-d-seen-feature)\n","   ##### 2.3.2 [YouTube dataset (Unseen)](#youtube-dataset-unseen-feature)\n","   ##### 2.3.3 [ESD](#esd-feature)\n","   ##### 2.3.3.1 [ESD (Seen)](#esd-seen-feature)\n","   ##### 2.3.3.2 [ESD (Unseen)](#esd-unseen-feature)\n","   ##### 2.3.4 [TESS dataset (Seen)](#tess-dataset-seen-feature)\n","   #### 2.4 [Combine Dataset (Seen)](#combine-dataset-seen)"]},{"cell_type":"markdown","metadata":{},"source":["## **2. Preprocessing the Data**<a id='preprocessing-the-data'></a>\n","\n","##### *Note: It is advisable to run this notebook on Python 3.9.6.*"]},{"cell_type":"markdown","metadata":{},"source":["### For our study, we will look at 4 datasets in total:\n","\n","#### 1) [Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D)](https://github.com/CheyneyComputerScience/CREMA-D)\n","    \n","CREMA-D is a data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified). Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).\n","\n","For the simplicity of the study, we will not look into the emotion levels as a feature/variable.\n","\n","Data can be downloaded [here](https://www.kaggle.com/datasets/ejlok1/cremad).\n","\n","#### 2) YouTube\n","\n","This includes audio data extracted from YouTube videos, documenting respondents sharing their experience with the mental health struggles. More details can be found in the [ReadMe](README.md) or [previous notebook](code/02_Preprocessing.ipynb). \n","\n","#### 3) [Toronto Emotional Speech Set (TESS)](https://tspace.library.utoronto.ca/handle/1807/24487)\n","\n","TESS consist of 2,800 2-sec audio clips, with a set of 200 target words were spoken in the carrier phrase \"Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral).\n","\n","The dataset is organised such that each of the two female actor and their emotions are contain within its own folder. And within that, all 200 target words audio file can be found. \n","\n","Data can be downloaded [here](https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess).\n","\n","#### 4) [Emotion Speech Dataset (ESD)](https://github.com/HLTSingapore/Emotional-Speech-Data)\n","\n","The ESD database has 35,000 2-sec audio clips, and was recorded with the aim to provide the community with a large emotional speech database with a sufficient variety of speaker and lexical coverage.\n","\n","The ESD database consists of a total of 29 hours of audio recordings from 10 native English speakers and 10 native Chinese speakers, covering 5 different emotion categories (neutral, happy, angry, sad and surprise). It represents one of the largest emotional speech databases publicly available, in terms of speakers and lexical variability. All the recordings are conducted in the studio with professional devices to guarantee audio quality.\n","\n","Data can be downloaded [here](https://drive.google.com/file/d/1Ypu1lQ5ThmslmCGhFZyq2vFkLmVqy9uS/view?usp=share_link).\n","\n","Thereafter, we will split them into 2 groups - Seen and Unseen Data. \n","* Seen Data - This includes the data that have been trained on before, in previous studies, and the labels are predefined by the data collectors. The reason for combining dataset is to ensure a more robust model training, and thereby prediction of emotions through speech.\n","    - CREMA-D\n","    - TESS\n","    - ESD \n","* Unseen Data - On the flipside, the unseen data is one that has never been trained on before. The only exception will be that the we'll use a pre-allocated evaluation data from ESD (`ESD_eval`) as a medium of validating our model, after predicting the YouTube data.\n","    - YouTube \n","    - `ESD_eval`"]},{"cell_type":"markdown","metadata":{},"source":["### Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import librosa\n","import numpy as np\n","import os\n","import pandas as pd\n","import soundfile as sf"]},{"cell_type":"markdown","metadata":{},"source":["### **2.1 Data Augmentation**<a id='data-augmentation'></a>\n","\n","Audio data Augmentation is important in especially our use case, where speech is analysed either offline or online, this is to ensure:\n","\n","1) Robustness: Audio augmentations introduce controlled variability into training data, which helps in developing models that are robust and can generalize well across different acoustic environments and recording conditions.\n","\n","2) Data Augmentation: Especially in cases where the amount of training data is limited, augmentations effectively increase the dataset size, providing more training examples and helping prevent overfitting.\n","\n","3) Variability and Generalization: By simulating various real-world conditions, augmentations ensure that the model can handle a wide range of audio inputs, making it more effective in practical applications.\n","\n","The type of audio augmentation we seek to introduce here is the injection of Noise, Time Stretch, Time Shifting and Pitch Shifting. The following is an explanation of each augmentation type, and what we have done in with our audio data.\n","\n","* Noise Addition (`noise(data)`)\n","    - This simulates real-world scenarios where background noise is present, training the model to be robust against noisy environments. We have added random noise to the audio signal. The amplitude of the noise is a fraction (up to 3.5%) of the maximum amplitude of the audio signal, ensuring that the noise level is substantial but not overwhelming.\n","\n","* Time Stretching (`stretch(data)`)\n","    - Helps the model learn to recognize emotions regardless of slight variations in speech speed, which can vary from person to person or due to emotional state. We stretche or compresses the audio signal by a random factor between 95% and 105% of the original length.\n","\n","* Time Shifting (`shift(data)`)\n","    - Introduces a variation in the temporal location of speech within the audio files, mimicking the effect of speaking earlier or later within a recording window. We have circularly shift the audio data by a random number of samples in the range of -300 to 300. \n","\n","* Pitch Shifting (`pitch(data, sr)`)\n","    - Accounts for variations in pitch, which can be influenced by the speaker's mood, age, gender, or emotional state, thereby training the model to be invariant to pitch changes. Here we modify the pitch of the audio signal by a random number of semitones, ranging from -2 to 2."]},{"cell_type":"markdown","metadata":{},"source":["#### **2.1.1 CREMA-D (Seen)**<a id='crema-d-seen'></a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../dataset/CREMA'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m input_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/CREMA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/CREMA_aug\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mprocess_crema_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[40], line 34\u001b[0m, in \u001b[0;36mprocess_crema_files\u001b[0;34m(input_folder, output_folder)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_folder):\n\u001b[1;32m     33\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_folder)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     36\u001b[0m         path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_folder, filename)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/CREMA'"]}],"source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)  # Reduced noise amplitude\n","    return data + noise_amp * np.random.normal(size=data.shape[0])\n","\n","def stretch(data):\n","    stretch_rate = np.random.uniform(low=0.95, high=1.05)  # Less aggressive range\n","    return librosa.effects.time_stretch(data, rate=stretch_rate)\n","\n","def shift(data):\n","    shift_range = int(np.random.uniform(low=-300, high=300))  # Reduced shift range\n","    return np.roll(data, shift_range)\n","\n","def pitch(data, sr):\n","    pitch_factor = np.random.uniform(low=-2, high=2)  # Less pitch variation\n","    return librosa.effects.pitch_shift(data, sr=sr, n_steps=pitch_factor)\n","\n","def augment_audio(file_path, output_folder, file_name):\n","    audio, sr = librosa.load(file_path, sr=None)\n","    audio = librosa.util.normalize(audio)\n","\n","    # Apply augmentations\n","    audio = noise(audio)\n","    audio = stretch(audio)\n","    audio = shift(audio)\n","    audio = pitch(audio, sr)\n","\n","    # Saving the augmented audio\n","    output_file_path = os.path.join(output_folder, f\"aug_{file_name}\")\n","    sf.write(output_file_path, audio, sr)\n","\n","def process_crema_files(input_folder, output_folder):\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","    for filename in os.listdir(input_folder):\n","        if filename.endswith(\".wav\"):\n","            path = os.path.join(input_folder, filename)\n","            augment_audio(path, output_folder, filename)\n","\n","input_folder = '../dataset/CREMA'\n","output_folder = '../dataset/CREMA_aug'\n","process_crema_files(input_folder, output_folder) # ignore the error (files will be provided in the dataset folder)"]},{"cell_type":"markdown","metadata":{},"source":["#### **2.1.2 YouTube dataset (Unseen)**<a id='youtube-dataset-unseen'></a>"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)  # Reduced noise amplitude\n","    return data + noise_amp * np.random.normal(size=data.shape[0])\n","\n","def stretch(data):\n","    stretch_rate = np.random.uniform(low=0.95, high=1.05)  # Less aggressive range\n","    return librosa.effects.time_stretch(data, rate=stretch_rate)\n","\n","def shift(data):\n","    shift_range = int(np.random.uniform(low=-300, high=300))  # Reduced shift range\n","    return np.roll(data, shift_range)\n","\n","def pitch(data, sr):\n","    pitch_factor = np.random.uniform(low=-2, high=2)  # Less pitch variation\n","    return librosa.effects.pitch_shift(data, sr=sr, n_steps=pitch_factor)\n","\n","def augment_audio(file_path, output_folder, file_name):\n","    audio, sr = librosa.load(file_path, sr=None)\n","    audio = librosa.util.normalize(audio)\n","\n","    # Apply augmentations\n","    audio = noise(audio)\n","    audio = stretch(audio)\n","    audio = shift(audio)\n","    audio = pitch(audio, sr)\n","\n","    # Saving the augmented audio\n","    output_file_path = os.path.join(output_folder, f\"aug_{file_name}\")\n","    sf.write(output_file_path, audio, sr)\n","\n","def process_audio_files(input_folder, output_folder):\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","    for filename in os.listdir(input_folder):\n","        if filename.endswith(\".wav\"):\n","            path = os.path.join(input_folder, filename)\n","            augment_audio(path, output_folder, filename)\n","\n","input_folder = '../dataset/YouTube'\n","output_folder = '../dataset/YouTube_aug'\n","process_audio_files(input_folder, output_folder)"]},{"cell_type":"markdown","metadata":{},"source":["#### **2.1.3 ESD data (Seen + Unseen)**<a id='esd-data-seen-unseen'></a>"]},{"cell_type":"markdown","metadata":{},"source":["##### **2.1.3.1 ESD (Seen)**<a id='esd-seen'></a>\n","\n","This includes the pre-allocated training and test data, of 33,000 original 2-sec clips."]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)  # Reduced noise amplitude\n","    return data + noise_amp * np.random.normal(size=data.shape[0])\n","\n","def stretch(data):\n","    stretch_rate = np.random.uniform(low=0.95, high=1.05)  # Less aggressive range\n","    return librosa.effects.time_stretch(data, rate=stretch_rate)\n","\n","def shift(data):\n","    shift_range = int(np.random.uniform(low=-300, high=300))  # Reduced shift range\n","    return np.roll(data, shift_range)\n","\n","def pitch(data, sr):\n","    pitch_factor = np.random.uniform(low=-2, high=2)  # Less pitch variation\n","    return librosa.effects.pitch_shift(data, sr=sr, n_steps=pitch_factor)\n","\n","def augment_audio(file_path, output_folder, file_name):\n","    audio, sr = librosa.load(file_path, sr=None)\n","    audio = librosa.util.normalize(audio)\n","\n","    # Apply augmentations\n","    audio = noise(audio)\n","    audio = stretch(audio)\n","    audio = shift(audio)\n","    audio = pitch(audio, sr)\n","\n","    # Saving the augmented audio\n","    output_file_path = os.path.join(output_folder, f\"aug_{file_name}\")\n","    sf.write(output_file_path, audio, sr)\n","\n","def process_audio_files(input_folder, output_folder):\n","    for actor_folder in os.listdir(input_folder):\n","        actor_path = os.path.join(input_folder, actor_folder)\n","        if os.path.isdir(actor_path):\n","            for emotion_folder in os.listdir(actor_path):\n","                emotion_path = os.path.join(actor_path, emotion_folder)\n","                if os.path.isdir(emotion_path):\n","                    output_emotion_folder = os.path.join(output_folder, actor_folder, emotion_folder)\n","                    if not os.path.exists(output_emotion_folder):\n","                        os.makedirs(output_emotion_folder)\n","                    for file in os.listdir(emotion_path):\n","                        if file.endswith(\".wav\"):\n","                            file_path = os.path.join(emotion_path, file)\n","                            augment_audio(file_path, output_emotion_folder, file)\n","\n","input_folder = '../dataset/ESD'\n","output_folder = '../dataset/ESD_aug'\n","process_audio_files(input_folder, output_folder)"]},{"cell_type":"markdown","metadata":{},"source":["##### **2.1.3.2 ESD (Unseen)**<a id='esd-unseen'></a>\n","\n","This includes the pre-allocated evaluation data, of 2,000 original 2-sec clips."]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)  # Reduced noise amplitude\n","    return data + noise_amp * np.random.normal(size=data.shape[0])\n","\n","def stretch(data):\n","    stretch_rate = np.random.uniform(low=0.95, high=1.05)  # Less aggressive range\n","    return librosa.effects.time_stretch(data, rate=stretch_rate)\n","\n","def shift(data):\n","    shift_range = int(np.random.uniform(low=-300, high=300))  # Reduced shift range\n","    return np.roll(data, shift_range)\n","\n","def pitch(data, sr):\n","    pitch_factor = np.random.uniform(low=-2, high=2)  # Less pitch variation\n","    return librosa.effects.pitch_shift(data, sr=sr, n_steps=pitch_factor)\n","\n","def augment_audio(file_path, output_folder, file_name):\n","    audio, sr = librosa.load(file_path, sr=None)\n","    audio = librosa.util.normalize(audio)\n","\n","    # Apply augmentations\n","    audio = noise(audio)\n","    audio = stretch(audio)\n","    audio = shift(audio)\n","    audio = pitch(audio, sr)\n","\n","    # Saving the augmented audio\n","    output_file_path = os.path.join(output_folder, f\"aug_{file_name}\")\n","    sf.write(output_file_path, audio, sr)\n","\n","def process_audio_files(input_folder, output_folder):\n","    for actor_folder in os.listdir(input_folder):\n","        actor_path = os.path.join(input_folder, actor_folder)\n","        if os.path.isdir(actor_path):\n","            for emotion_folder in os.listdir(actor_path):\n","                emotion_path = os.path.join(actor_path, emotion_folder)\n","                if os.path.isdir(emotion_path):\n","                    output_emotion_folder = os.path.join(output_folder, actor_folder, emotion_folder)\n","                    if not os.path.exists(output_emotion_folder):\n","                        os.makedirs(output_emotion_folder)\n","                    for file in os.listdir(emotion_path):\n","                        if file.endswith(\".wav\"):\n","                            file_path = os.path.join(emotion_path, file)\n","                            augment_audio(file_path, output_emotion_folder, file)\n","\n","input_folder = '../dataset/ESD_eval'\n","output_folder = '../dataset/ESD_eval_aug'\n","process_audio_files(input_folder, output_folder)"]},{"cell_type":"markdown","metadata":{},"source":["#### **2.1.4 TESS dataset (Seen)**<a id='tess-dataset-seen'></a>"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)  # Adjusted noise amplitude\n","    return data + noise_amp * np.random.normal(size=data.shape[0])\n","\n","def stretch(data):\n","    stretch_rate = np.random.uniform(low=0.95, high=1.05)  # Adjusted stretch range\n","    return librosa.effects.time_stretch(data, rate=stretch_rate)\n","\n","def shift(data):\n","    shift_range = int(np.random.uniform(low=-300, high=300))  # Adjusted shift range\n","    return np.roll(data, shift_range)\n","\n","def pitch(data, sr):\n","    pitch_factor = np.random.uniform(low=-2, high=2)  # Adjusted pitch variation\n","    return librosa.effects.pitch_shift(data, sr=sr, n_steps=pitch_factor)\n","\n","def augment_audio(file_path, output_folder, file_name):\n","    audio, sr = librosa.load(file_path, sr=None)\n","    audio = librosa.util.normalize(audio)\n","\n","    # Apply augmentations\n","    audio = noise(audio)\n","    audio = stretch(audio)\n","    audio = shift(audio)\n","    audio = pitch(audio, sr)\n","\n","    # Saving the augmented audio\n","    output_file_path = os.path.join(output_folder, f\"aug_{file_name}\")\n","    sf.write(output_file_path, audio, sr)\n","\n","def process_tess_files(input_folder, output_folder):\n","    for age_group in os.listdir(input_folder):  # 'OAF' or 'YAF'\n","        age_path = os.path.join(input_folder, age_group)\n","        if os.path.isdir(age_path):\n","            for emotion_folder in os.listdir(age_path):\n","                emotion_path = os.path.join(age_path, emotion_folder)\n","                if os.path.isdir(emotion_path):\n","                    output_emotion_folder = os.path.join(output_folder, age_group, emotion_folder)\n","                    if not os.path.exists(output_emotion_folder):\n","                        os.makedirs(output_emotion_folder)\n","                    for file in os.listdir(emotion_path):\n","                        if file.endswith(\".wav\"):\n","                            file_path = os.path.join(emotion_path, file)\n","                            augment_audio(file_path, output_emotion_folder, file)\n","\n","# Set the input and output folders for TESS\n","input_folder = '../dataset/TESS'\n","output_folder = '../dataset/TESS_aug'\n","process_tess_files(input_folder, output_folder)"]},{"cell_type":"markdown","metadata":{},"source":["### **2.2 Label Mapping**<a id='label-mapping'></a>\n","\n","Although most of the datasets (except YouTube) already have pre-defined labels, their labels are may vary to each other.\n","\n","This section aims to standardise the mapping of the emotion labels to the CREMA-D. The rationale for choosing this is because the CREMA-D data provides a breadth of emotions commonly expressed by people. Emotions like 'Surprise' or 'Pleasant Surprise' - where evident in ESD and TESS respectively - do not fully capture an emotion type. Hence, we mapped it to the closest we thought it would be - `Neutral` and `Happy` respectively.\n","\n","We will only be Label Mapping the *augmented* Seen Data and the `ESD_eval` (Unseen Data)."]},{"cell_type":"markdown","metadata":{},"source":["#### **2.2.1 CREMA-D (Seen)**<a id='crema-d-seen-label'></a>"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Define the directory containing the CREMA dataset\n","crema_directory = '../dataset/CREMA_aug'\n","\n","# List all files in the directory\n","crema_directory_list = os.listdir(crema_directory)\n","\n","# Lists to hold file emotions and file paths\n","file_emotion = []\n","file_path = []\n","\n","# Populate the lists\n","for file in crema_directory_list:\n","    # Construct the full file path\n","    full_path = os.path.join(crema_directory, file)\n","    file_path.append(full_path)\n","\n","    # Extract the emotion part from the filename\n","    parts = file.split('_')\n","    if len(parts) > 3:\n","        emotion_code = parts[3][:3]  # The emotion code is typically at this position\n","        emotion = {\n","            'NEU': 'Neutral',\n","            'ANG': 'Angry',\n","            'SAD': 'Sad',\n","            'FEA': 'Fear',\n","            'HAP': 'Happy',\n","            'DIS': 'Disgust'\n","        }.get(emotion_code, 'Unknown')  # Default to 'Unknown' if not found\n","        file_emotion.append(emotion)\n","    else:\n","        file_emotion.append('Unknown')\n","\n","# Create DataFrames from the lists\n","emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n","path_df = pd.DataFrame(file_path, columns=['Path'])\n","\n","# Concatenate the DataFrames to a single DataFrame\n","crema_df = pd.concat([emotion_df, path_df], axis=1)\n","# Extract only the filename from the full path and update the 'Path' column\n","crema_df['Path'] = crema_df['Path'].apply(lambda x: os.path.basename(x))\n","\n","# Save the DataFrame to a CSV file\n","crema_df.to_csv('../csv/CREMA_Dataset.csv', index=False)"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Emotions</th>\n","      <th>Path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Happy</td>\n","      <td>aug_1075_TAI_HAP_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sad</td>\n","      <td>aug_1051_IEO_SAD_MD.wav</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sad</td>\n","      <td>aug_1044_IEO_SAD_MD.wav</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Happy</td>\n","      <td>aug_1060_TAI_HAP_XX.wav</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sad</td>\n","      <td>aug_1005_IWL_SAD_XX.wav</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Emotions                     Path\n","0    Happy  aug_1075_TAI_HAP_XX.wav\n","1      Sad  aug_1051_IEO_SAD_MD.wav\n","2      Sad  aug_1044_IEO_SAD_MD.wav\n","3    Happy  aug_1060_TAI_HAP_XX.wav\n","4      Sad  aug_1005_IWL_SAD_XX.wav"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["crema_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["#### **2.2.2 ESD**<a id='esd-label'></a>"]},{"cell_type":"markdown","metadata":{},"source":["##### **2.2.2.1 ESD (Seen)**<a id='esd-seen-label'></a>"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["# Label Mapping for the emotions in the ESD dataset\n","label_mapping = {\n","    'Angry': 'Angry',\n","    'Happy': 'Happy',\n","    'Neutral': 'Neutral',\n","    'Sad': 'Sad',\n","    'Surprise': 'Neutral'  # Mapping 'Surprise' to 'Neutral'\n","}\n","\n","def map_emotion(emotion_folder):\n","    return label_mapping.get(emotion_folder, 'Unknown')\n","\n","def process_audio_files(input_folder, output_folder):\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    file_emotion = []\n","    file_path = []\n","\n","    for root, dirs, files in os.walk(input_folder):\n","        for file in files:\n","            if file.endswith(\".wav\"):\n","                full_path = os.path.join(root, file)\n","                file_path.append(full_path)\n","\n","                # Extract the emotion part from the directory structure\n","                parts = root.split(os.sep)\n","                emotion_folder = parts[-1]  # Assuming the emotion is the last part of the path\n","                mapped_emotion = map_emotion(emotion_folder)\n","                file_emotion.append(mapped_emotion)\n","\n","                if mapped_emotion == 'Unknown':\n","                    print(f\"Unmapped emotion from path: {root}\")  # Debug: print unmapped path\n","\n","    # Create DataFrames from the lists\n","    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n","    path_df = pd.DataFrame(file_path, columns=['Path'])\n","\n","    # Concatenate the DataFrames into a single DataFrame\n","    esd_df = pd.concat([emotion_df, path_df], axis=1)\n","\n","    # Save the DataFrame to a CSV file\n","    esd_df.to_csv(f'{output_folder}/ESD_Dataset.csv', index=False)\n","\n","# Define paths\n","input_folder = '../dataset/ESD_aug'  # Make sure this is the correct path\n","output_folder = '../csv'\n","process_audio_files(input_folder, output_folder)\n","\n","# Load the generated file to check its content\n","esd_df = pd.read_csv(f'{output_folder}/ESD_Dataset.csv')\n","print(esd_df.head())"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Emotions</th>\n","      <th>Path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Happy</td>\n","      <td>../dataset/ESD_aug/0003/Happy/aug_0003_000957.wav</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Happy</td>\n","      <td>../dataset/ESD_aug/0003/Happy/aug_0003_000943.wav</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Happy</td>\n","      <td>../dataset/ESD_aug/0003/Happy/aug_0003_000994.wav</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Happy</td>\n","      <td>../dataset/ESD_aug/0003/Happy/aug_0003_000758.wav</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Happy</td>\n","      <td>../dataset/ESD_aug/0003/Happy/aug_0003_000980.wav</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Emotions                                               Path\n","0    Happy  ../dataset/ESD_aug/0003/Happy/aug_0003_000957.wav\n","1    Happy  ../dataset/ESD_aug/0003/Happy/aug_0003_000943.wav\n","2    Happy  ../dataset/ESD_aug/0003/Happy/aug_0003_000994.wav\n","3    Happy  ../dataset/ESD_aug/0003/Happy/aug_0003_000758.wav\n","4    Happy  ../dataset/ESD_aug/0003/Happy/aug_0003_000980.wav"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["esd_df = pd.read_csv(f'{output_folder}/ESD_Dataset.csv')\n","esd_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["##### **2.2.2.2 ESD (Unseen)**<a id='esd-unseen-label'></a>"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["# Label Mapping for the emotions in the ESD_eval dataset\n","label_mapping = {\n","    'Angry': 'Angry',\n","    'Happy': 'Happy',\n","    'Neutral': 'Neutral',\n","    'Sad': 'Sad',\n","    'Surprise': 'Neutral'\n","}\n","\n","def map_emotion(emotion_folder):\n","    return label_mapping.get(emotion_folder, 'Unknown')\n","\n","def process_audio_files(input_folder, output_folder):\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    file_emotion = []\n","    file_path = []\n","\n","    for root, dirs, files in os.walk(input_folder):\n","        for file in files:\n","            if file.endswith(\".wav\"):\n","                full_path = os.path.join(root, file)\n","                file_path.append(full_path)\n","\n","                # Extract the emotion part from the directory structure\n","                parts = root.split(os.sep)\n","                emotion_folder = parts[-1]  # Assuming the emotion is the last part of the path\n","                mapped_emotion = map_emotion(emotion_folder)\n","                file_emotion.append(mapped_emotion)\n","\n","                if mapped_emotion == 'Unknown':\n","                    print(f\"Unmapped emotion from path: {root}\")  # Debug: print unmapped path\n","\n","    # Create DataFrames from the lists\n","    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n","    path_df = pd.DataFrame(file_path, columns=['Path'])\n","\n","    # Concatenate the DataFrames into a single DataFrame\n","    esd_eval_df = pd.concat([emotion_df, path_df], axis=1)\n","\n","    # Save the DataFrame to a CSV file\n","    esd_eval_df.to_csv(f'{output_folder}/ESD_eval_Dataset.csv', index=False)\n","\n","# Define paths\n","input_folder = '../dataset/ESD_eval_aug'  # Make sure this is the correct path\n","output_folder = '../csv'\n","process_audio_files(input_folder, output_folder)"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Emotions</th>\n","      <th>Path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Happy</td>\n","      <td>../dataset/ESD_eval_aug/0003/Happy/aug_0003_00...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Happy</td>\n","      <td>../dataset/ESD_eval_aug/0003/Happy/aug_0003_00...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Happy</td>\n","      <td>../dataset/ESD_eval_aug/0003/Happy/aug_0003_00...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Happy</td>\n","      <td>../dataset/ESD_eval_aug/0003/Happy/aug_0003_00...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Happy</td>\n","      <td>../dataset/ESD_eval_aug/0003/Happy/aug_0003_00...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Emotions                                               Path\n","0    Happy  ../dataset/ESD_eval_aug/0003/Happy/aug_0003_00...\n","1    Happy  ../dataset/ESD_eval_aug/0003/Happy/aug_0003_00...\n","2    Happy  ../dataset/ESD_eval_aug/0003/Happy/aug_0003_00...\n","3    Happy  ../dataset/ESD_eval_aug/0003/Happy/aug_0003_00...\n","4    Happy  ../dataset/ESD_eval_aug/0003/Happy/aug_0003_00..."]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["esd_eval_df = pd.read_csv(f'{output_folder}/ESD_eval_Dataset.csv')\n","esd_eval_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["#### **2.2.3 TESS dataset (Seen)**<a id='tess-dataset-seen-label'></a>"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["# Label Mapping for the emotions in the TESS dataset\n","label_mapping = {\n","    'angry': 'Angry',\n","    'happy': 'Happy',\n","    'neutral': 'Neutral',\n","    'sad': 'Sad',\n","    'disgust': 'Disgust',\n","    'fear': 'Fear',\n","    'pleasantsurprise': 'Happy'  # Assuming 'ps' stands for pleasant surprise\n","}\n","\n","def map_emotion(emotion_folder):\n","    # Extract the emotion part and map it\n","    emotion_part = emotion_folder.split('_')[-1].lower()  # Get the last part after '_'\n","    return label_mapping.get(emotion_part, 'Unknown')\n","\n","def process_audio_files(input_folder, output_folder):\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    file_emotion = []\n","    file_path = []\n","\n","    for root, dirs, files in os.walk(input_folder):\n","        for file in files:\n","            if file.endswith(\".wav\"):\n","                full_path = os.path.join(root, file)\n","                file_path.append(full_path)\n","\n","                # Extract the emotion part from the directory structure\n","                parts = root.split(os.sep)\n","                emotion_folder = parts[-2] + '_' + parts[-1]  # Combining voice type and emotion\n","                mapped_emotion = map_emotion(emotion_folder)\n","                file_emotion.append(mapped_emotion)\n","\n","                if mapped_emotion == 'Unknown':\n","                    print(f\"Unmapped emotion from path: {root}\")  # Debug: print unmapped path\n","\n","    # Create DataFrames from the lists\n","    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n","    path_df = pd.DataFrame(file_path, columns=['Path'])\n","\n","    # Concatenate the DataFrames into a single DataFrame\n","    tess_df = pd.concat([emotion_df, path_df], axis=1)\n","\n","    # Save the DataFrame to a CSV file\n","    tess_df.to_csv(f'{output_folder}/TESS_Dataset.csv', index=False)\n","\n","# Define paths\n","input_folder = '../dataset/TESS_aug'  # Adjust as necessary\n","output_folder = '../csv'\n","process_audio_files(input_folder, output_folder)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Emotions</th>\n","      <th>Path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Happy</td>\n","      <td>../dataset/TESS_aug/OAF/OAF_happy/aug_OAF_vine...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Happy</td>\n","      <td>../dataset/TESS_aug/OAF/OAF_happy/aug_OAF_seiz...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Happy</td>\n","      <td>../dataset/TESS_aug/OAF/OAF_happy/aug_OAF_bar_...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Happy</td>\n","      <td>../dataset/TESS_aug/OAF/OAF_happy/aug_OAF_door...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Happy</td>\n","      <td>../dataset/TESS_aug/OAF/OAF_happy/aug_OAF_nice...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Emotions                                               Path\n","0    Happy  ../dataset/TESS_aug/OAF/OAF_happy/aug_OAF_vine...\n","1    Happy  ../dataset/TESS_aug/OAF/OAF_happy/aug_OAF_seiz...\n","2    Happy  ../dataset/TESS_aug/OAF/OAF_happy/aug_OAF_bar_...\n","3    Happy  ../dataset/TESS_aug/OAF/OAF_happy/aug_OAF_door...\n","4    Happy  ../dataset/TESS_aug/OAF/OAF_happy/aug_OAF_nice..."]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["tess_df = pd.read_csv(f'{output_folder}/TESS_Dataset.csv')\n","tess_df.head()"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["# Combine the CREMA, TESS, ESD datasets (Label Mapping) \n","combined_df_map = pd.concat([crema_df, tess_df, esd_df], ignore_index=True)\n","combined_df_map.to_csv('../csv/combined_Dataset.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### **2.3 Feature Extraction**<a id='feature-extraction'></a>\n","\n","The extracted functions look at extracting from the previous augmentation types. The extracted functions are as follows:\n","* Zero-Crossing Rate (1 value)\n","    - The rate at which the signal changes from positive to negative or back. This is a simple measure of the frequency content of a signal and is often used to distinguish percussive sounds from harmonic sounds. The output is typically a single mean value per time frame (or the entire audio clip if you average across frames), which represents the average rate of zero-crossings.\n","\n","* Chroma Short-Time Fourier Transform or STFT (12 values)\n","    - Captures the essence of music pitches, ignoring aspects like timbre and loudness. Chroma refers to the 12 different pitch classes; each feature in a chromagram corresponds to one of the twelve distinct semitones (or notes) in an octave.\n","\n","* Mel-Frequency Cepstral Coefficients - MFCC (20 values)\n","    - Represents the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency. The number of MFCCs you choose to compute can vary, but 20 is a common choice in many applications, providing a good trade-off between capturing important aspects of the sound and computational efficiency. Each MFCC captures different characteristics of the sound, and together they provide a compact representation of the spectral envelope.\n","\n","* Root Mean Square (1 value)\n","    - It is the measure of the average power or amplitude of the audio signal, essentially representing the 'loudness' of the sound. The output for RMS is usually a single value per frame which can be averaged across all frames to give a single measure of the overall signal strength. Hence, there is only 1 value.\n","\n","* Spectral Contrast (7 values)\n","    - Measures the dynamic range of the spectrum within different sub-bands. By analyzing the contrast between the most prominent tones and the less intense sounds that surround them, spectral contrast provides a measure of the perceptual quality of the sound and can be useful for distinguishing different types of sound textures and timbres. The choice of seven features for spectral contrast generally follows the common practice of dividing the audible spectrum into seven sub-bands, which aligns with perceptually meaningful frequency ranges that correspond to different musical or speech attributes. "]},{"cell_type":"markdown","metadata":{},"source":["#### **2.3.1 CREMA-D (Seen)**<a id='crema-d-seen-feature'></a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/amoz/.pyenv/versions/myenv38/lib/python3.8/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n","  return pitch_tuning(\n"]}],"source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n","    data = data + noise_amp * np.random.normal(size=data.shape[0])\n","    return data\n","\n","def stretch(data):\n","    stretch_rate = np.random.uniform(low=0.95, high=1.05) \n","    return librosa.effects.time_stretch(data, rate=stretch_rate)\n","\n","def pitch(data, sr, pitch_factor=2):\n","    return librosa.effects.pitch_shift(data, sr=sr, n_steps=pitch_factor)\n","\n","def extract_features(data, sample_rate):\n","    n_fft = 2048\n","    hop_length = 512\n","    # Collect features for different modifications of the data\n","    features = []\n","    transformations = [lambda x: x, noise, stretch, lambda x: pitch(x, sample_rate, 2)]\n","    for transform in transformations:\n","        modified_data = transform(data)\n","        #ZCR\n","        zcr = librosa.feature.zero_crossing_rate(y=modified_data, hop_length=hop_length).T.mean(axis=0)\n","        #Chroma SFTF\n","        chroma_stft = librosa.feature.chroma_stft(y=modified_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        #MFCC\n","        mfcc = librosa.feature.mfcc(y=modified_data, sr=sample_rate, n_mfcc=20, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        #RMS\n","        rms = librosa.feature.rms(y=modified_data, hop_length=hop_length).T.mean(axis=0)\n","        #Spectral Contrast\n","        spectral_contrast = librosa.feature.spectral_contrast(y=modified_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        feature_array = np.hstack((zcr, chroma_stft, mfcc, rms, spectral_contrast))\n","        features.append(feature_array)\n","    # Flatten the list of arrays into a single array\n","    return np.hstack(features)\n","\n","def generate_feature_names():\n","    features_info = {\n","        'ZCR': 1,\n","        'Chroma': 12,\n","        'MFCC': 20,\n","        'RMS': 1,\n","        'Spectral_Contrast': 7\n","    }\n","    types = ['original', 'noisy', 'stretched', 'pitched']\n","    names = []\n","    for feat_name, count in features_info.items():\n","        for i in range(1, count + 1):\n","            for t in types:\n","                names.append(f\"{feat_name}_{i}_{t}\")\n","    return names\n","\n","def map_emotion(emotion_code):\n","    emotions = {'NEU': 'Neutral', 'ANG': 'Angry', 'SAD': 'Sad', 'FEA': 'Fear', 'HAP': 'Happy', 'DIS': 'Disgust'}\n","    return emotions.get(emotion_code, 'Unknown')\n","\n","def process_files_crema(directory):\n","    features_list = []\n","    labels = []\n","    filenames = [f for f in os.listdir(directory) if f.endswith(\".wav\")]\n","    for filename in filenames:\n","        file_path = os.path.join(directory, filename)\n","        data, sample_rate = librosa.load(file_path, duration=2.5, offset=0.6)\n","        original_features = extract_features(data, sample_rate)\n","        features_list.append(original_features)\n","        parts = filename.split('_')\n","        emotion_code = parts[3]  # Adjust filename pattern as needed\n","        labels.append(map_emotion(emotion_code))\n","    feature_names = generate_feature_names()\n","    features_df = pd.DataFrame(features_list, columns=feature_names)\n","    features_df['Label'] = labels\n","    return features_df\n","\n","# Define path to the dataset\n","crema_dataset_path = '../dataset/CREMA_aug'\n","crema_features_df = process_files_crema(crema_dataset_path)\n","\n","# Save to CSV\n","crema_features_df.to_csv('../csv/CREMA_aug_features.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ZCR_1_original</th>\n","      <th>ZCR_1_noisy</th>\n","      <th>ZCR_1_stretched</th>\n","      <th>ZCR_1_pitched</th>\n","      <th>Chroma_1_original</th>\n","      <th>Chroma_1_noisy</th>\n","      <th>Chroma_1_stretched</th>\n","      <th>Chroma_1_pitched</th>\n","      <th>Chroma_2_original</th>\n","      <th>Chroma_2_noisy</th>\n","      <th>...</th>\n","      <th>Spectral_Contrast_5_pitched</th>\n","      <th>Spectral_Contrast_6_original</th>\n","      <th>Spectral_Contrast_6_noisy</th>\n","      <th>Spectral_Contrast_6_stretched</th>\n","      <th>Spectral_Contrast_6_pitched</th>\n","      <th>Spectral_Contrast_7_original</th>\n","      <th>Spectral_Contrast_7_noisy</th>\n","      <th>Spectral_Contrast_7_stretched</th>\n","      <th>Spectral_Contrast_7_pitched</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.058203</td>\n","      <td>0.168413</td>\n","      <td>0.140260</td>\n","      <td>0.222031</td>\n","      <td>0.205495</td>\n","      <td>0.329408</td>\n","      <td>0.406689</td>\n","      <td>0.287099</td>\n","      <td>0.210801</td>\n","      <td>0.316133</td>\n","      <td>...</td>\n","      <td>5.502777</td>\n","      <td>0.054518</td>\n","      <td>15.121599</td>\n","      <td>16.979465</td>\n","      <td>22.924174</td>\n","      <td>23.677009</td>\n","      <td>22.195865</td>\n","      <td>19.999263</td>\n","      <td>65.080437</td>\n","      <td>Happy</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.049544</td>\n","      <td>0.248724</td>\n","      <td>0.239492</td>\n","      <td>0.279569</td>\n","      <td>0.269511</td>\n","      <td>0.267364</td>\n","      <td>0.369389</td>\n","      <td>0.340688</td>\n","      <td>0.429027</td>\n","      <td>0.748550</td>\n","      <td>...</td>\n","      <td>-15.320172</td>\n","      <td>0.059525</td>\n","      <td>14.036520</td>\n","      <td>14.780670</td>\n","      <td>15.586150</td>\n","      <td>15.448796</td>\n","      <td>16.005200</td>\n","      <td>16.309107</td>\n","      <td>66.911804</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.048374</td>\n","      <td>0.390639</td>\n","      <td>0.236833</td>\n","      <td>0.227314</td>\n","      <td>0.248309</td>\n","      <td>0.256225</td>\n","      <td>0.355961</td>\n","      <td>0.268141</td>\n","      <td>0.288257</td>\n","      <td>0.298744</td>\n","      <td>...</td>\n","      <td>-6.954707</td>\n","      <td>0.050574</td>\n","      <td>15.823275</td>\n","      <td>15.035871</td>\n","      <td>17.236838</td>\n","      <td>17.331110</td>\n","      <td>16.973861</td>\n","      <td>17.195164</td>\n","      <td>65.261740</td>\n","      <td>Sad</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.125309</td>\n","      <td>0.464438</td>\n","      <td>0.444223</td>\n","      <td>0.343996</td>\n","      <td>0.322230</td>\n","      <td>0.334812</td>\n","      <td>0.260258</td>\n","      <td>0.340670</td>\n","      <td>0.333610</td>\n","      <td>0.319384</td>\n","      <td>...</td>\n","      <td>-9.659683</td>\n","      <td>0.039167</td>\n","      <td>13.341967</td>\n","      <td>18.273013</td>\n","      <td>19.682507</td>\n","      <td>20.911843</td>\n","      <td>17.991627</td>\n","      <td>17.974893</td>\n","      <td>69.016132</td>\n","      <td>Happy</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.129747</td>\n","      <td>0.342628</td>\n","      <td>0.373324</td>\n","      <td>0.495987</td>\n","      <td>0.503508</td>\n","      <td>0.380763</td>\n","      <td>0.363083</td>\n","      <td>0.435024</td>\n","      <td>0.391251</td>\n","      <td>0.419177</td>\n","      <td>...</td>\n","      <td>-12.033329</td>\n","      <td>0.025420</td>\n","      <td>18.242384</td>\n","      <td>14.344610</td>\n","      <td>18.553798</td>\n","      <td>17.762759</td>\n","      <td>17.671194</td>\n","      <td>16.586530</td>\n","      <td>70.563453</td>\n","      <td>Sad</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 165 columns</p>\n","</div>"],"text/plain":["   ZCR_1_original  ZCR_1_noisy  ZCR_1_stretched  ZCR_1_pitched  \\\n","0        0.058203     0.168413         0.140260       0.222031   \n","1        0.049544     0.248724         0.239492       0.279569   \n","2        0.048374     0.390639         0.236833       0.227314   \n","3        0.125309     0.464438         0.444223       0.343996   \n","4        0.129747     0.342628         0.373324       0.495987   \n","\n","   Chroma_1_original  Chroma_1_noisy  Chroma_1_stretched  Chroma_1_pitched  \\\n","0           0.205495        0.329408            0.406689          0.287099   \n","1           0.269511        0.267364            0.369389          0.340688   \n","2           0.248309        0.256225            0.355961          0.268141   \n","3           0.322230        0.334812            0.260258          0.340670   \n","4           0.503508        0.380763            0.363083          0.435024   \n","\n","   Chroma_2_original  Chroma_2_noisy  ...  Spectral_Contrast_5_pitched  \\\n","0           0.210801        0.316133  ...                     5.502777   \n","1           0.429027        0.748550  ...                   -15.320172   \n","2           0.288257        0.298744  ...                    -6.954707   \n","3           0.333610        0.319384  ...                    -9.659683   \n","4           0.391251        0.419177  ...                   -12.033329   \n","\n","   Spectral_Contrast_6_original  Spectral_Contrast_6_noisy  \\\n","0                      0.054518                  15.121599   \n","1                      0.059525                  14.036520   \n","2                      0.050574                  15.823275   \n","3                      0.039167                  13.341967   \n","4                      0.025420                  18.242384   \n","\n","   Spectral_Contrast_6_stretched  Spectral_Contrast_6_pitched  \\\n","0                      16.979465                    22.924174   \n","1                      14.780670                    15.586150   \n","2                      15.035871                    17.236838   \n","3                      18.273013                    19.682507   \n","4                      14.344610                    18.553798   \n","\n","   Spectral_Contrast_7_original  Spectral_Contrast_7_noisy  \\\n","0                     23.677009                  22.195865   \n","1                     15.448796                  16.005200   \n","2                     17.331110                  16.973861   \n","3                     20.911843                  17.991627   \n","4                     17.762759                  17.671194   \n","\n","   Spectral_Contrast_7_stretched  Spectral_Contrast_7_pitched  Label  \n","0                      19.999263                    65.080437  Happy  \n","1                      16.309107                    66.911804    Sad  \n","2                      17.195164                    65.261740    Sad  \n","3                      17.974893                    69.016132  Happy  \n","4                      16.586530                    70.563453    Sad  \n","\n","[5 rows x 165 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["crema_features_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["     ZCR_1_original  ZCR_1_noisy  ZCR_1_stretched  ZCR_1_pitched  \\\n","min        0.000000     0.000000         0.000000       0.000000   \n","max        0.330717     0.916395         0.821496       0.874335   \n","\n","     Chroma_1_original  Chroma_1_noisy  Chroma_1_stretched  Chroma_1_pitched  \\\n","min           0.000000        0.000000            0.000000          0.000000   \n","max           0.810113        0.793729            0.842867          0.898079   \n","\n","     Chroma_2_original  Chroma_2_noisy  ...  Spectral_Contrast_5_stretched  \\\n","min           0.000000        0.000000  ...                     -16.374187   \n","max           0.840144        0.955284  ...                      22.734421   \n","\n","     Spectral_Contrast_5_pitched  Spectral_Contrast_6_original  \\\n","min                   -21.689333                      0.000000   \n","max                    17.644751                      0.142532   \n","\n","     Spectral_Contrast_6_noisy  Spectral_Contrast_6_stretched  \\\n","min                   0.000000                       0.000000   \n","max                  24.768058                      21.890491   \n","\n","     Spectral_Contrast_6_pitched  Spectral_Contrast_7_original  \\\n","min                      0.00000                      0.000000   \n","max                     26.93428                     25.572385   \n","\n","     Spectral_Contrast_7_noisy  Spectral_Contrast_7_stretched  \\\n","min                   0.000000                       0.000000   \n","max                  25.294995                      24.871831   \n","\n","     Spectral_Contrast_7_pitched  \n","min                     0.000000  \n","max                    74.026823  \n","\n","[2 rows x 164 columns]\n"]}],"source":["crema_features_df = pd.read_csv('../csv/CREMA_aug_features.csv')\n","\n","feature_ranges = crema_features_df.describe().loc[['min', 'max']]\n","\n","print(feature_ranges)"]},{"cell_type":"markdown","metadata":{},"source":["#### **2.3.2 YouTube dataset (Unseen)**<a id='youtube-dataset-unseen-feature'></a>"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/amoz/.pyenv/versions/myenv38/lib/python3.8/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n","  return pitch_tuning(\n"]}],"source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n","    data = data + noise_amp * np.random.normal(size=data.shape[0])\n","    return data\n","\n","def stretch(data):\n","    stretch_rate = np.random.uniform(low=0.95, high=1.05) \n","    return librosa.effects.time_stretch(data, rate=stretch_rate)\n","\n","def pitch(data, sr, pitch_factor=2):\n","    return librosa.effects.pitch_shift(data, sr=sr, n_steps=pitch_factor)\n","\n","def extract_features(data, sample_rate):\n","    n_fft = 2048\n","    hop_length = 512\n","    # Collect features for different modifications of the data\n","    features = []\n","    transformations = [lambda x: x, noise, stretch, lambda x: pitch(x, sample_rate, 2)]\n","    for transform in transformations:\n","            modified_data = transform(data)\n","            #ZCR\n","            zcr = librosa.feature.zero_crossing_rate(y=modified_data, hop_length=hop_length).T.mean(axis=0)\n","            #Chroma SFTF\n","            chroma_stft = librosa.feature.chroma_stft(y=modified_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","            #MFCC\n","            mfcc = librosa.feature.mfcc(y=modified_data, sr=sample_rate, n_mfcc=20, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","            #RMS\n","            rms = librosa.feature.rms(y=modified_data, hop_length=hop_length).T.mean(axis=0)\n","            #Spectral Contrast\n","            spectral_contrast = librosa.feature.spectral_contrast(y=modified_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","            feature_array = np.hstack((zcr, chroma_stft, mfcc, rms, spectral_contrast))\n","            features.append(feature_array)\n","    # Flatten the list of arrays into a single array\n","    return np.hstack(features)\n","\n","def generate_feature_names():\n","    features_info = {\n","        'ZCR': 1,\n","        'Chroma': 12,\n","        'MFCC': 20,\n","        'RMS': 1,\n","        'Spectral_Contrast': 7\n","    }\n","    types = ['original', 'noisy', 'stretched', 'pitched']\n","    names = []\n","    for feat_name, count in features_info.items():\n","        for i in range(1, count + 1):\n","            for t in types:\n","                names.append(f\"{feat_name}_{i}_{t}\")\n","    return names\n","\n","def process_files_youtube(directory):\n","    features_list = []\n","    filenames = [f for f in os.listdir(directory) if f.endswith(\".wav\")]\n","    for filename in filenames:\n","        file_path = os.path.join(directory, filename)\n","        data, sample_rate = librosa.load(file_path, duration=2.5, offset=0.6)\n","        features = extract_features(data, sample_rate)\n","        features_list.append(features)\n","    feature_names = generate_feature_names()\n","    features_df = pd.DataFrame(features_list, columns=feature_names)\n","    return features_df\n","\n","# Define path to the dataset for the unseen YouTube data\n","dataset_path_youtube = '../dataset/YouTube_aug'\n","features_df_youtube = process_files_youtube(dataset_path_youtube)\n","\n","# Save to CSV\n","features_df_youtube.to_csv('../csv/YouTube_aug_features.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["     ZCR_1_original  ZCR_1_noisy  ZCR_1_stretched  ZCR_1_pitched  \\\n","min        0.000000     0.000000         0.000000       0.000000   \n","max        0.609013     0.876495         0.929679       0.902725   \n","\n","     Chroma_1_original  Chroma_1_noisy  Chroma_1_stretched  Chroma_1_pitched  \\\n","min           0.000000        0.000000            0.000000           0.00000   \n","max           0.918657        0.925917            0.990164           0.97337   \n","\n","     Chroma_2_original  Chroma_2_noisy  ...  Spectral_Contrast_5_stretched  \\\n","min           0.000000        0.000000  ...                     -20.488947   \n","max           0.928489        0.885999  ...                      12.891306   \n","\n","     Spectral_Contrast_5_pitched  Spectral_Contrast_6_original  \\\n","min                   -17.072622                      0.000000   \n","max                    14.112740                      0.153067   \n","\n","     Spectral_Contrast_6_noisy  Spectral_Contrast_6_stretched  \\\n","min                   0.000000                        0.00000   \n","max                  31.145072                       25.28959   \n","\n","     Spectral_Contrast_6_pitched  Spectral_Contrast_7_original  \\\n","min                     0.000000                      0.000000   \n","max                    29.061982                     29.957943   \n","\n","     Spectral_Contrast_7_noisy  Spectral_Contrast_7_stretched  \\\n","min                    0.00000                        0.00000   \n","max                   25.93541                       30.87265   \n","\n","     Spectral_Contrast_7_pitched  \n","min                     0.000000  \n","max                    51.898679  \n","\n","[2 rows x 164 columns]\n"]}],"source":["yt_features_df = pd.read_csv('../csv/YouTube_aug_features.csv')\n","\n","feature_ranges = yt_features_df.describe().loc[['min', 'max']]\n","\n","print(feature_ranges)"]},{"cell_type":"markdown","metadata":{},"source":["#### **2.3.3 ESD**<a id='esd-feature'></a>"]},{"cell_type":"markdown","metadata":{},"source":["##### **2.3.3.1 ESD (Seen)**<a id='esd-seen-feature'></a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n","    data = data + noise_amp * np.random.normal(size=data.shape[0])\n","    return data\n","\n","def stretch(data):\n","    stretch_rate = np.random.uniform(low=0.95, high=1.05) \n","    return librosa.effects.time_stretch(data, rate=stretch_rate)\n","\n","def pitch(data, sr, pitch_factor=2):\n","    return librosa.effects.pitch_shift(data, sr=sr, n_steps=pitch_factor)\n","\n","def extract_features(data, sample_rate):\n","    n_fft = 2048\n","    hop_length = 512\n","    features = []\n","    transformations = [lambda x: x, noise, stretch, lambda x: pitch(x, sample_rate, 2)]\n","    for transform in transformations:\n","        modified_data = transform(data)\n","        zcr = librosa.feature.zero_crossing_rate(y=modified_data, hop_length=hop_length).T.mean(axis=0)\n","        chroma_stft = librosa.feature.chroma_stft(y=modified_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        mfcc = librosa.feature.mfcc(y=modified_data, sr=sample_rate, n_mfcc=20, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        rms = librosa.feature.rms(y=modified_data, hop_length=hop_length).T.mean(axis=0)\n","        spectral_contrast = librosa.feature.spectral_contrast(y=modified_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        feature_array = np.hstack((zcr, chroma_stft, mfcc, rms, spectral_contrast))\n","        features.append(feature_array)\n","    return np.hstack(features)\n","\n","def generate_feature_names():\n","    features_info = {'ZCR': 1, 'Chroma': 12, 'MFCC': 20, 'RMS': 1, 'Spectral_Contrast': 7}\n","    types = ['original', 'noisy', 'stretched', 'pitched']\n","    names = []\n","    for t in types:\n","        for feat_name, count in features_info.items():\n","            for i in range(1, count + 1):\n","                names.append(f\"{feat_name}_{i}_{t}\")\n","    return names\n","\n","def map_emotion(directory_name):\n","    emotion_mapping = {'Angry': 'Angry', 'Happy': 'Happy', 'Neutral': 'Neutral', 'Sad': 'Sad', 'Surprise': 'Happy'}\n","    return emotion_mapping.get(directory_name, 'Unknown')\n","\n","def process_files_esd(directory):\n","    features_list = []\n","    labels = []\n","    for root, _, files in os.walk(directory):\n","        for filename in files:\n","            if filename.endswith(\".wav\"):\n","                file_path = os.path.join(root, filename)\n","                data, sample_rate = librosa.load(file_path, duration=2.5, offset=0.6)\n","                features = extract_features(data, sample_rate)\n","                features_list.append(features)\n","                labels.append(map_emotion(root.split(os.sep)[-1]))\n","    feature_names = generate_feature_names()  \n","    features_df = pd.DataFrame(features_list, columns=feature_names)\n","    features_df['Label'] = labels\n","    return features_df\n","\n","input_folder_esd = '../dataset/ESD_aug'\n","features_df_esd = process_files_esd(input_folder_esd)\n","features_df_esd.to_csv('../csv/ESD_aug_features.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["     ZCR_1_original  Chroma_1_original  Chroma_2_original  Chroma_3_original  \\\n","min        0.035283           0.022347           0.039505           0.045234   \n","max        0.362988           0.839597           0.888793           0.818959   \n","\n","     Chroma_4_original  Chroma_5_original  Chroma_6_original  \\\n","min           0.032947           0.033066           0.026778   \n","max           0.812716           0.861563           0.939885   \n","\n","     Chroma_7_original  Chroma_8_original  Chroma_9_original  ...  \\\n","min           0.025347           0.016972           0.021873  ...   \n","max           0.869625           0.930398           0.871389  ...   \n","\n","     MFCC_19_pitched  MFCC_20_pitched  RMS_1_pitched  \\\n","min       -14.547981       -24.669319       0.007123   \n","max        45.623779        30.724112       0.138693   \n","\n","     Spectral_Contrast_1_pitched  Spectral_Contrast_2_pitched  \\\n","min                     9.907291                    11.455714   \n","max                    36.115294                    31.504129   \n","\n","     Spectral_Contrast_3_pitched  Spectral_Contrast_4_pitched  \\\n","min                    13.344527                    13.357976   \n","max                    31.044957                    29.559362   \n","\n","     Spectral_Contrast_5_pitched  Spectral_Contrast_6_pitched  \\\n","min                    14.392380                    14.144437   \n","max                    26.231437                    28.314595   \n","\n","     Spectral_Contrast_7_pitched  \n","min                    56.499374  \n","max                    74.907225  \n","\n","[2 rows x 164 columns]\n"]}],"source":["esd_features_df = pd.read_csv('../csv/ESD_aug_features.csv')\n","\n","feature_ranges = esd_features_df.describe().loc[['min', 'max']]\n","\n","print(feature_ranges)"]},{"cell_type":"markdown","metadata":{},"source":["##### **2.3.3.2 ESD (Unseen)**<a id='esd-unseen-feature'></a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n","    data = data + noise_amp * np.random.normal(size=data.shape[0])\n","    return data\n","\n","def stretch(data):\n","    stretch_rate = np.random.uniform(low=0.95, high=1.05) \n","    return librosa.effects.time_stretch(data, rate=stretch_rate)\n","\n","def pitch(data, sr, pitch_factor=2):\n","    return librosa.effects.pitch_shift(data, sr=sr, n_steps=pitch_factor)\n","\n","def extract_features(data, sample_rate):\n","    n_fft = 2048\n","    hop_length = 512\n","    features = []\n","    transformations = [lambda x: x, noise, stretch, lambda x: pitch(x, sample_rate, 2)]\n","    for transform in transformations:\n","        modified_data = transform(data)\n","        zcr = librosa.feature.zero_crossing_rate(y=modified_data, hop_length=hop_length).T.mean(axis=0)\n","        chroma_stft = librosa.feature.chroma_stft(y=modified_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        mfcc = librosa.feature.mfcc(y=modified_data, sr=sample_rate, n_mfcc=20, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        rms = librosa.feature.rms(y=modified_data, hop_length=hop_length).T.mean(axis=0)\n","        spectral_contrast = librosa.feature.spectral_contrast(y=modified_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        feature_array = np.hstack((zcr, chroma_stft, mfcc, rms, spectral_contrast))\n","        features.append(feature_array)\n","    return np.hstack(features)\n","\n","def generate_feature_names():\n","    features_info = {'ZCR': 1, 'Chroma': 12, 'MFCC': 20, 'RMS': 1, 'Spectral_Contrast': 7}\n","    types = ['original', 'noisy', 'stretched', 'pitched']\n","    names = []\n","    for t in types:\n","        for feat_name, count in features_info.items():\n","            for i in range(1, count + 1):\n","                names.append(f\"{feat_name}_{i}_{t}\")\n","    return names\n","\n","def map_emotion(directory_name):\n","    emotion_mapping = {'Angry': 'Angry', 'Happy': 'Happy', 'Neutral': 'Neutral', 'Sad': 'Sad', 'Surprise': 'Happy'}\n","    return emotion_mapping.get(directory_name, 'Unknown')\n","\n","def process_files_esd(directory):\n","    features_list = []\n","    labels = []\n","    for root, _, files in os.walk(directory):\n","        for filename in files:\n","            if filename.endswith(\".wav\"):\n","                file_path = os.path.join(root, filename)\n","                data, sample_rate = librosa.load(file_path, duration=2.5, offset=0.6)\n","                features = extract_features(data, sample_rate)\n","                features_list.append(features)\n","                labels.append(map_emotion(root.split(os.sep)[-1]))\n","    feature_names = generate_feature_names()  \n","    features_df = pd.DataFrame(features_list, columns=feature_names)\n","    features_df['Label'] = labels\n","    return features_df\n","\n","input_folder_esd = '../dataset/ESD_eval_aug'\n","features_df_esd = process_files_esd(input_folder_esd)\n","features_df_esd.to_csv('../csv/ESD_eval_aug_features.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["     ZCR_1_original  Chroma_1_original  Chroma_2_original  Chroma_3_original  \\\n","min        0.051016           0.065614           0.085547           0.067468   \n","max        0.338617           0.849386           0.765298           0.754705   \n","\n","     Chroma_4_original  Chroma_5_original  Chroma_6_original  \\\n","min           0.016535           0.018146           0.031143   \n","max           0.826904           0.744975           0.769989   \n","\n","     Chroma_7_original  Chroma_8_original  Chroma_9_original  ...  \\\n","min           0.032557           0.031282           0.054921  ...   \n","max           0.775975           0.834109           0.810564  ...   \n","\n","     MFCC_19_pitched  MFCC_20_pitched  RMS_1_pitched  \\\n","min       -14.683578       -18.586187       0.012181   \n","max        25.541897        16.717937       0.101416   \n","\n","     Spectral_Contrast_1_pitched  Spectral_Contrast_2_pitched  \\\n","min                    10.900086                    11.970423   \n","max                    31.750141                    26.341261   \n","\n","     Spectral_Contrast_3_pitched  Spectral_Contrast_4_pitched  \\\n","min                    14.863704                    13.898533   \n","max                    27.423123                    27.025439   \n","\n","     Spectral_Contrast_5_pitched  Spectral_Contrast_6_pitched  \\\n","min                    15.281096                     14.93828   \n","max                    24.931240                     26.55847   \n","\n","     Spectral_Contrast_7_pitched  \n","min                    58.992660  \n","max                    74.058071  \n","\n","[2 rows x 164 columns]\n"]}],"source":["esd_eval_features_df = pd.read_csv('../csv/ESD_eval_aug_features.csv')\n","\n","feature_ranges = esd_eval_features_df.describe().loc[['min', 'max']]\n","\n","print(feature_ranges)"]},{"cell_type":"markdown","metadata":{},"source":["#### **2.3.4 TESS dataset (Seen)**<a id='tess-dataset-seen-feature'></a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def noise(data):\n","    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n","    return data + noise_amp * np.random.normal(size=data.shape[0])\n","\n","def stretch(data):\n","    stretch_rate = np.random.uniform(low=0.95, high=1.05)\n","    return librosa.effects.time_stretch(data, rate=stretch_rate)\n","\n","def pitch(data, sr):\n","    pitch_factor = np.random.uniform(low=-2, high=2)\n","    return librosa.effects.pitch_shift(data, sr=sr, n_steps=pitch_factor)\n","\n","def extract_features(data, sample_rate):\n","    n_fft = 2048\n","    hop_length = 512\n","    transformations = [lambda x: x, noise, stretch, lambda x: pitch(x, sample_rate)]\n","    features = []\n","    for transform in transformations:\n","        modified_data = transform(data)\n","        zcr = librosa.feature.zero_crossing_rate(modified_data, hop_length=hop_length).T.mean(axis=0)\n","        chroma_stft = librosa.feature.chroma_stft(y=modified_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        mfcc = librosa.feature.mfcc(y=modified_data, sr=sample_rate, n_mfcc=20, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        rms = librosa.feature.rms(y=modified_data, hop_length=hop_length).T.mean(axis=0)\n","        spectral_contrast = librosa.feature.spectral_contrast(y=modified_data, sr=sample_rate, n_fft=n_fft, hop_length=hop_length).T.mean(axis=0)\n","        feature_array = np.hstack((zcr, chroma_stft, mfcc, rms, spectral_contrast))\n","        features.append(feature_array)\n","    return np.hstack(features)\n","\n","def generate_feature_names():\n","    features_info = {'ZCR': 1, 'Chroma': 12, 'MFCC': 20, 'RMS': 1, 'Spectral_Contrast': 7}\n","    types = ['original', 'noisy', 'stretched', 'pitched']\n","    names = []\n","    for t in types:\n","        for feature_name, count in features_info.items():\n","            for i in range(1, count + 1):\n","                names.append(f\"{feature_name}_{i}_{t}\")\n","    return names\n","\n","def map_emotion(audio_file_name):\n","    emotion_keywords = {\n","        'angry': 'Angry',\n","        'happy': 'Happy',\n","        'neutral': 'Neutral',\n","        'sad': 'Sad',\n","        'disgust': 'Disgust',\n","        'fear': 'Fear',\n","        'ps': 'Happy'\n","    }\n","    for key in emotion_keywords:\n","        if key in audio_file_name:\n","            return emotion_keywords[key]\n","    return 'Unknown'\n","\n","def process_files_tess(directory):\n","    features_list = []\n","    labels = []\n","    feature_names = generate_feature_names()\n","    for root, _, files in os.walk(directory):\n","        for file in files:\n","            if file.endswith(\".wav\"):\n","                file_path = os.path.join(root, file)\n","                data, sample_rate = librosa.load(file_path, sr=None)\n","                features = extract_features(data, sample_rate)\n","                features_list.append(features)\n","                label = map_emotion(file)\n","                labels.append(label)\n","    features_df = pd.DataFrame(features_list, columns=feature_names)\n","    features_df['Label'] = labels\n","    return features_df\n","\n","input_folder_tess = '../dataset/TESS_aug'\n","features_df_tess = process_files_tess(input_folder_tess)\n","features_df_tess.to_csv('../csv/TESS_aug_features.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["     ZCR_1_original  Chroma_1_original  Chroma_2_original  Chroma_3_original  \\\n","min        0.053142           0.080655           0.092351           0.057976   \n","max        0.356191           0.813709           0.758412           0.824369   \n","\n","     Chroma_4_original  Chroma_5_original  Chroma_6_original  \\\n","min           0.059733           0.058978           0.046615   \n","max           0.815024           0.838086           0.850368   \n","\n","     Chroma_7_original  Chroma_8_original  Chroma_9_original  ...  \\\n","min           0.043898           0.029222           0.029962  ...   \n","max           0.899741           0.917119           0.930549  ...   \n","\n","     MFCC_19_pitched  MFCC_20_pitched  RMS_1_pitched  \\\n","min       -19.988125       -12.419239       0.013126   \n","max        22.311520        33.925964       0.208303   \n","\n","     Spectral_Contrast_1_pitched  Spectral_Contrast_2_pitched  \\\n","min                    10.230365                    13.027649   \n","max                    30.393639                    29.286084   \n","\n","     Spectral_Contrast_3_pitched  Spectral_Contrast_4_pitched  \\\n","min                    15.495713                    15.227454   \n","max                    33.526235                    28.234872   \n","\n","     Spectral_Contrast_5_pitched  Spectral_Contrast_6_pitched  \\\n","min                    14.521892                    14.458459   \n","max                    25.775449                    26.398419   \n","\n","     Spectral_Contrast_7_pitched  \n","min                    38.727356  \n","max                    72.473887  \n","\n","[2 rows x 164 columns]\n"]}],"source":["tess_features_df = pd.read_csv('../csv/TESS_aug_features.csv')\n","\n","feature_ranges = tess_features_df.describe().loc[['min', 'max']]\n","\n","print(feature_ranges)"]},{"cell_type":"markdown","metadata":{},"source":["### **2.4 Combine Dataset (Seen)**<a id='combine-dataset-seen'></a>\n","\n","For the purpose of training and testing, we will combine the seen data together. The pros of using a diverse set of data is as follows:\n","\n","1. Increased Diversity\n","Each dataset typically captures a variety of emotional expressions under different conditions and from different demographics. For example:\n","\n","    * CREMA-D: This dataset is known for its diversity in terms of actor demographics and has a variety of vocal expressions recorded in a controlled studio environment.\n","    * TESS (Toronto Emotional Speech Set): It features a range of emotions spoken by Canadian English speakers, often with a focus on older adults.\n","    * ESD (Emotional Speech Dataset): It might offer different linguistic backgrounds or recording conditions.\n","\n","Combining these datasets ensures that the model is exposed to a broader spectrum of vocal qualities, accents, intonations, and speech nuances, which can differ substantially across datasets due to geographical, cultural, and individual speaker differences.\n","\n","2. Robustness and Generalizability\n","\n","    * Avoid Overfitting: Training and testing on a single dataset can sometimes lead models to overfit to the specific characteristics and idiosyncrasies of that dataset. Using multiple datasets can help ensure that the model performs well across a more generalized set of data conditions, not just the one it was trained on.\n","    * Real-World Application: In practical applications, a model may encounter a wide range of speech inputs from users of different ages, ethnic backgrounds, and emotional states. Testing the model across different datasets helps verify that it can handle this variability effectively.\n","\n","3. Enhanced Validation and Testing\n","    * Cross-Dataset Validation: Models can be validated more rigorously by using one dataset for training and others for testing. This cross-dataset testing helps to highlight any biases the model may have towards the training data.\n","    * Reliability and Accuracy: By testing on multiple datasets, you can assess the reliability and accuracy of the model across different types of emotional speech data, which is crucial for applications like clinical diagnosis, customer service, and interactive AI.\n","\n","4. Statistical Significance\n","    * Increased Sample Size: More data points can lead to more statistically significant results and conclusions. It reduces the impact of outliers and allows for more complex modeling techniques that require larger datasets.\n","    * Variety of Testing Scenarios: It enables testing the model under different scenarios and conditions, which can be critical for ensuring the modelâ€™s utility in real-world applications."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine the features from the CREMA, TESS, ESD \n","combined_df = pd.concat([crema_features_df, esd_features_df, tess_features_df], ignore_index=True)\n","combined_df.to_csv('../csv/combined_aug_features.csv', index=False)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}
